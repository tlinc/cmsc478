{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMSC478 Machine Learning\n",
    "\n",
    "# Assignment-2: Classification with Logistic Regression and SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Type your name and ID here* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview and Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've learnt many new topics and concepts in the last 3 chapters (chapter 3, 4, and 5). You learned various performance evaluation strategies that you can use to evaluate your classification and regression models. You also learned that Support Vector Machines (SVM) is an effective algorithm for solving a wide range of ML problems.\n",
    "\n",
    "In Part I of this assignment, you are going to build and compare three classifiers applied on the breast cancer dataset. You will also perform fine-tuning the parameters. As an <font color=\"green\">  extra credit</font> opportunity in Part II, you train a SVM classifier for image classification.\n",
    "\n",
    "Pedagogically, this assignment will help you:\n",
    "- better understand SVM, as well as logistic regression.\n",
    "\n",
    "- how to perform error analysis, parameter settings and model selection.\n",
    "\n",
    "- practice plotting techniques using matplotlib.\n",
    "\n",
    "- pratice reading documentation. This is a very important skill in AI/ML/Data Science collaborative environments and teams.\n",
    "\n",
    "So, let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I - Classification with SGD, Logistic Regression, and SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Assignment-1, you built a SGD classifier with a good accuracy on the breast cancer dataset. You are now going to build two more classifiers that can be applied on the same datset using the following algorithms: Logistic Regression, and Support Vector Machines (SVM).\n",
    "\n",
    "- You will compare the three classifiers SGD, Logistic Regression and SVM with respect to their parameters and three accuracy measures: f score, auc (ROC), and cross validation.\n",
    "\n",
    "- You will use a grid-search strategy (or brute-force, i.e. trial and error) to fine-tune the parameters of your classifiers.\n",
    "\n",
    "- You will use matplotlib to plot your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary Python modules\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data = datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the type of the variable `breast_cancer` which holds our data is `Bunch` and is a dictionary-like data structure, we extract features and labels and explore their shape to better understand the data. This version of `breast_cancer` dataset has thirty features which are specifications of the tumor as well as two classes/labels `['malignant' 'benign']`.\n",
    "\n",
    "The data set consists of 569 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and labels\n",
    "label_names = cancer_data['target_names']\n",
    "labels = cancer_data['target']\n",
    "feature_names = cancer_data['feature_names']\n",
    "features = cancer_data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['malignant' 'benign']\n",
      "0 0\n",
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
      "[1.799e+01 1.038e+01 1.228e+02 1.001e+03 1.184e-01 2.776e-01 3.001e-01\n",
      " 1.471e-01 2.419e-01 7.871e-02 1.095e+00 9.053e-01 8.589e+00 1.534e+02\n",
      " 6.399e-03 4.904e-02 5.373e-02 1.587e-02 3.003e-02 6.193e-03 2.538e+01\n",
      " 1.733e+01 1.846e+02 2.019e+03 1.622e-01 6.656e-01 7.119e-01 2.654e-01\n",
      " 4.601e-01 1.189e-01]\n"
     ]
    }
   ],
   "source": [
    "print(label_names)\n",
    "print(labels[0], labels[1])\n",
    "print(feature_names)\n",
    "print(features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use pandas to load the data as a dataframe, and then we can use pandas methods to visualize our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_df = pd.DataFrame(cancer_data.data, columns = cancer_data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.990</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.300100</td>\n",
       "      <td>0.147100</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.71190</td>\n",
       "      <td>0.26540</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.570</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.086900</td>\n",
       "      <td>0.070170</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.24160</td>\n",
       "      <td>0.18600</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.690</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.197400</td>\n",
       "      <td>0.127900</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.45040</td>\n",
       "      <td>0.24300</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.420</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.241400</td>\n",
       "      <td>0.105200</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.68690</td>\n",
       "      <td>0.25750</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.290</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>0.104300</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.40000</td>\n",
       "      <td>0.16250</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.450</td>\n",
       "      <td>15.70</td>\n",
       "      <td>82.57</td>\n",
       "      <td>477.1</td>\n",
       "      <td>0.12780</td>\n",
       "      <td>0.17000</td>\n",
       "      <td>0.157800</td>\n",
       "      <td>0.080890</td>\n",
       "      <td>0.2087</td>\n",
       "      <td>0.07613</td>\n",
       "      <td>...</td>\n",
       "      <td>15.470</td>\n",
       "      <td>23.75</td>\n",
       "      <td>103.40</td>\n",
       "      <td>741.6</td>\n",
       "      <td>0.17910</td>\n",
       "      <td>0.52490</td>\n",
       "      <td>0.53550</td>\n",
       "      <td>0.17410</td>\n",
       "      <td>0.3985</td>\n",
       "      <td>0.12440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18.250</td>\n",
       "      <td>19.98</td>\n",
       "      <td>119.60</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>0.09463</td>\n",
       "      <td>0.10900</td>\n",
       "      <td>0.112700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.1794</td>\n",
       "      <td>0.05742</td>\n",
       "      <td>...</td>\n",
       "      <td>22.880</td>\n",
       "      <td>27.66</td>\n",
       "      <td>153.20</td>\n",
       "      <td>1606.0</td>\n",
       "      <td>0.14420</td>\n",
       "      <td>0.25760</td>\n",
       "      <td>0.37840</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.3063</td>\n",
       "      <td>0.08368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13.710</td>\n",
       "      <td>20.83</td>\n",
       "      <td>90.20</td>\n",
       "      <td>577.9</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.16450</td>\n",
       "      <td>0.093660</td>\n",
       "      <td>0.059850</td>\n",
       "      <td>0.2196</td>\n",
       "      <td>0.07451</td>\n",
       "      <td>...</td>\n",
       "      <td>17.060</td>\n",
       "      <td>28.14</td>\n",
       "      <td>110.60</td>\n",
       "      <td>897.0</td>\n",
       "      <td>0.16540</td>\n",
       "      <td>0.36820</td>\n",
       "      <td>0.26780</td>\n",
       "      <td>0.15560</td>\n",
       "      <td>0.3196</td>\n",
       "      <td>0.11510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13.000</td>\n",
       "      <td>21.82</td>\n",
       "      <td>87.50</td>\n",
       "      <td>519.8</td>\n",
       "      <td>0.12730</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.185900</td>\n",
       "      <td>0.093530</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>0.07389</td>\n",
       "      <td>...</td>\n",
       "      <td>15.490</td>\n",
       "      <td>30.73</td>\n",
       "      <td>106.20</td>\n",
       "      <td>739.3</td>\n",
       "      <td>0.17030</td>\n",
       "      <td>0.54010</td>\n",
       "      <td>0.53900</td>\n",
       "      <td>0.20600</td>\n",
       "      <td>0.4378</td>\n",
       "      <td>0.10720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12.460</td>\n",
       "      <td>24.04</td>\n",
       "      <td>83.97</td>\n",
       "      <td>475.9</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.23960</td>\n",
       "      <td>0.227300</td>\n",
       "      <td>0.085430</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.08243</td>\n",
       "      <td>...</td>\n",
       "      <td>15.090</td>\n",
       "      <td>40.68</td>\n",
       "      <td>97.65</td>\n",
       "      <td>711.4</td>\n",
       "      <td>0.18530</td>\n",
       "      <td>1.05800</td>\n",
       "      <td>1.10500</td>\n",
       "      <td>0.22100</td>\n",
       "      <td>0.4366</td>\n",
       "      <td>0.20750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16.020</td>\n",
       "      <td>23.24</td>\n",
       "      <td>102.70</td>\n",
       "      <td>797.8</td>\n",
       "      <td>0.08206</td>\n",
       "      <td>0.06669</td>\n",
       "      <td>0.032990</td>\n",
       "      <td>0.033230</td>\n",
       "      <td>0.1528</td>\n",
       "      <td>0.05697</td>\n",
       "      <td>...</td>\n",
       "      <td>19.190</td>\n",
       "      <td>33.88</td>\n",
       "      <td>123.80</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>0.11810</td>\n",
       "      <td>0.15510</td>\n",
       "      <td>0.14590</td>\n",
       "      <td>0.09975</td>\n",
       "      <td>0.2948</td>\n",
       "      <td>0.08452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>15.780</td>\n",
       "      <td>17.89</td>\n",
       "      <td>103.60</td>\n",
       "      <td>781.0</td>\n",
       "      <td>0.09710</td>\n",
       "      <td>0.12920</td>\n",
       "      <td>0.099540</td>\n",
       "      <td>0.066060</td>\n",
       "      <td>0.1842</td>\n",
       "      <td>0.06082</td>\n",
       "      <td>...</td>\n",
       "      <td>20.420</td>\n",
       "      <td>27.28</td>\n",
       "      <td>136.50</td>\n",
       "      <td>1299.0</td>\n",
       "      <td>0.13960</td>\n",
       "      <td>0.56090</td>\n",
       "      <td>0.39650</td>\n",
       "      <td>0.18100</td>\n",
       "      <td>0.3792</td>\n",
       "      <td>0.10480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>19.170</td>\n",
       "      <td>24.80</td>\n",
       "      <td>132.40</td>\n",
       "      <td>1123.0</td>\n",
       "      <td>0.09740</td>\n",
       "      <td>0.24580</td>\n",
       "      <td>0.206500</td>\n",
       "      <td>0.111800</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07800</td>\n",
       "      <td>...</td>\n",
       "      <td>20.960</td>\n",
       "      <td>29.94</td>\n",
       "      <td>151.70</td>\n",
       "      <td>1332.0</td>\n",
       "      <td>0.10370</td>\n",
       "      <td>0.39030</td>\n",
       "      <td>0.36390</td>\n",
       "      <td>0.17670</td>\n",
       "      <td>0.3176</td>\n",
       "      <td>0.10230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15.850</td>\n",
       "      <td>23.95</td>\n",
       "      <td>103.70</td>\n",
       "      <td>782.7</td>\n",
       "      <td>0.08401</td>\n",
       "      <td>0.10020</td>\n",
       "      <td>0.099380</td>\n",
       "      <td>0.053640</td>\n",
       "      <td>0.1847</td>\n",
       "      <td>0.05338</td>\n",
       "      <td>...</td>\n",
       "      <td>16.840</td>\n",
       "      <td>27.66</td>\n",
       "      <td>112.00</td>\n",
       "      <td>876.5</td>\n",
       "      <td>0.11310</td>\n",
       "      <td>0.19240</td>\n",
       "      <td>0.23220</td>\n",
       "      <td>0.11190</td>\n",
       "      <td>0.2809</td>\n",
       "      <td>0.06287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13.730</td>\n",
       "      <td>22.61</td>\n",
       "      <td>93.60</td>\n",
       "      <td>578.3</td>\n",
       "      <td>0.11310</td>\n",
       "      <td>0.22930</td>\n",
       "      <td>0.212800</td>\n",
       "      <td>0.080250</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.07682</td>\n",
       "      <td>...</td>\n",
       "      <td>15.030</td>\n",
       "      <td>32.01</td>\n",
       "      <td>108.80</td>\n",
       "      <td>697.7</td>\n",
       "      <td>0.16510</td>\n",
       "      <td>0.77250</td>\n",
       "      <td>0.69430</td>\n",
       "      <td>0.22080</td>\n",
       "      <td>0.3596</td>\n",
       "      <td>0.14310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14.540</td>\n",
       "      <td>27.54</td>\n",
       "      <td>96.73</td>\n",
       "      <td>658.8</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.15950</td>\n",
       "      <td>0.163900</td>\n",
       "      <td>0.073640</td>\n",
       "      <td>0.2303</td>\n",
       "      <td>0.07077</td>\n",
       "      <td>...</td>\n",
       "      <td>17.460</td>\n",
       "      <td>37.13</td>\n",
       "      <td>124.10</td>\n",
       "      <td>943.2</td>\n",
       "      <td>0.16780</td>\n",
       "      <td>0.65770</td>\n",
       "      <td>0.70260</td>\n",
       "      <td>0.17120</td>\n",
       "      <td>0.4218</td>\n",
       "      <td>0.13410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14.680</td>\n",
       "      <td>20.13</td>\n",
       "      <td>94.74</td>\n",
       "      <td>684.5</td>\n",
       "      <td>0.09867</td>\n",
       "      <td>0.07200</td>\n",
       "      <td>0.073950</td>\n",
       "      <td>0.052590</td>\n",
       "      <td>0.1586</td>\n",
       "      <td>0.05922</td>\n",
       "      <td>...</td>\n",
       "      <td>19.070</td>\n",
       "      <td>30.88</td>\n",
       "      <td>123.40</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>0.14640</td>\n",
       "      <td>0.18710</td>\n",
       "      <td>0.29140</td>\n",
       "      <td>0.16090</td>\n",
       "      <td>0.3029</td>\n",
       "      <td>0.08216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16.130</td>\n",
       "      <td>20.68</td>\n",
       "      <td>108.10</td>\n",
       "      <td>798.8</td>\n",
       "      <td>0.11700</td>\n",
       "      <td>0.20220</td>\n",
       "      <td>0.172200</td>\n",
       "      <td>0.102800</td>\n",
       "      <td>0.2164</td>\n",
       "      <td>0.07356</td>\n",
       "      <td>...</td>\n",
       "      <td>20.960</td>\n",
       "      <td>31.48</td>\n",
       "      <td>136.80</td>\n",
       "      <td>1315.0</td>\n",
       "      <td>0.17890</td>\n",
       "      <td>0.42330</td>\n",
       "      <td>0.47840</td>\n",
       "      <td>0.20730</td>\n",
       "      <td>0.3706</td>\n",
       "      <td>0.11420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19.810</td>\n",
       "      <td>22.15</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>0.09831</td>\n",
       "      <td>0.10270</td>\n",
       "      <td>0.147900</td>\n",
       "      <td>0.094980</td>\n",
       "      <td>0.1582</td>\n",
       "      <td>0.05395</td>\n",
       "      <td>...</td>\n",
       "      <td>27.320</td>\n",
       "      <td>30.88</td>\n",
       "      <td>186.80</td>\n",
       "      <td>2398.0</td>\n",
       "      <td>0.15120</td>\n",
       "      <td>0.31500</td>\n",
       "      <td>0.53720</td>\n",
       "      <td>0.23880</td>\n",
       "      <td>0.2768</td>\n",
       "      <td>0.07615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13.540</td>\n",
       "      <td>14.36</td>\n",
       "      <td>87.46</td>\n",
       "      <td>566.3</td>\n",
       "      <td>0.09779</td>\n",
       "      <td>0.08129</td>\n",
       "      <td>0.066640</td>\n",
       "      <td>0.047810</td>\n",
       "      <td>0.1885</td>\n",
       "      <td>0.05766</td>\n",
       "      <td>...</td>\n",
       "      <td>15.110</td>\n",
       "      <td>19.26</td>\n",
       "      <td>99.70</td>\n",
       "      <td>711.2</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.17730</td>\n",
       "      <td>0.23900</td>\n",
       "      <td>0.12880</td>\n",
       "      <td>0.2977</td>\n",
       "      <td>0.07259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>13.080</td>\n",
       "      <td>15.71</td>\n",
       "      <td>85.63</td>\n",
       "      <td>520.0</td>\n",
       "      <td>0.10750</td>\n",
       "      <td>0.12700</td>\n",
       "      <td>0.045680</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>0.1967</td>\n",
       "      <td>0.06811</td>\n",
       "      <td>...</td>\n",
       "      <td>14.500</td>\n",
       "      <td>20.49</td>\n",
       "      <td>96.09</td>\n",
       "      <td>630.5</td>\n",
       "      <td>0.13120</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.18900</td>\n",
       "      <td>0.07283</td>\n",
       "      <td>0.3184</td>\n",
       "      <td>0.08183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9.504</td>\n",
       "      <td>12.44</td>\n",
       "      <td>60.34</td>\n",
       "      <td>273.9</td>\n",
       "      <td>0.10240</td>\n",
       "      <td>0.06492</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020760</td>\n",
       "      <td>0.1815</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>...</td>\n",
       "      <td>10.230</td>\n",
       "      <td>15.66</td>\n",
       "      <td>65.13</td>\n",
       "      <td>314.9</td>\n",
       "      <td>0.13240</td>\n",
       "      <td>0.11480</td>\n",
       "      <td>0.08867</td>\n",
       "      <td>0.06227</td>\n",
       "      <td>0.2450</td>\n",
       "      <td>0.07773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15.340</td>\n",
       "      <td>14.26</td>\n",
       "      <td>102.50</td>\n",
       "      <td>704.4</td>\n",
       "      <td>0.10730</td>\n",
       "      <td>0.21350</td>\n",
       "      <td>0.207700</td>\n",
       "      <td>0.097560</td>\n",
       "      <td>0.2521</td>\n",
       "      <td>0.07032</td>\n",
       "      <td>...</td>\n",
       "      <td>18.070</td>\n",
       "      <td>19.08</td>\n",
       "      <td>125.10</td>\n",
       "      <td>980.9</td>\n",
       "      <td>0.13900</td>\n",
       "      <td>0.59540</td>\n",
       "      <td>0.63050</td>\n",
       "      <td>0.23930</td>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.09946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>21.160</td>\n",
       "      <td>23.04</td>\n",
       "      <td>137.20</td>\n",
       "      <td>1404.0</td>\n",
       "      <td>0.09428</td>\n",
       "      <td>0.10220</td>\n",
       "      <td>0.109700</td>\n",
       "      <td>0.086320</td>\n",
       "      <td>0.1769</td>\n",
       "      <td>0.05278</td>\n",
       "      <td>...</td>\n",
       "      <td>29.170</td>\n",
       "      <td>35.59</td>\n",
       "      <td>188.00</td>\n",
       "      <td>2615.0</td>\n",
       "      <td>0.14010</td>\n",
       "      <td>0.26000</td>\n",
       "      <td>0.31550</td>\n",
       "      <td>0.20090</td>\n",
       "      <td>0.2822</td>\n",
       "      <td>0.07526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>16.650</td>\n",
       "      <td>21.38</td>\n",
       "      <td>110.00</td>\n",
       "      <td>904.6</td>\n",
       "      <td>0.11210</td>\n",
       "      <td>0.14570</td>\n",
       "      <td>0.152500</td>\n",
       "      <td>0.091700</td>\n",
       "      <td>0.1995</td>\n",
       "      <td>0.06330</td>\n",
       "      <td>...</td>\n",
       "      <td>26.460</td>\n",
       "      <td>31.56</td>\n",
       "      <td>177.00</td>\n",
       "      <td>2215.0</td>\n",
       "      <td>0.18050</td>\n",
       "      <td>0.35780</td>\n",
       "      <td>0.46950</td>\n",
       "      <td>0.20950</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.09564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>17.140</td>\n",
       "      <td>16.40</td>\n",
       "      <td>116.00</td>\n",
       "      <td>912.7</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.22760</td>\n",
       "      <td>0.222900</td>\n",
       "      <td>0.140100</td>\n",
       "      <td>0.3040</td>\n",
       "      <td>0.07413</td>\n",
       "      <td>...</td>\n",
       "      <td>22.250</td>\n",
       "      <td>21.40</td>\n",
       "      <td>152.40</td>\n",
       "      <td>1461.0</td>\n",
       "      <td>0.15450</td>\n",
       "      <td>0.39490</td>\n",
       "      <td>0.38530</td>\n",
       "      <td>0.25500</td>\n",
       "      <td>0.4066</td>\n",
       "      <td>0.10590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>14.580</td>\n",
       "      <td>21.53</td>\n",
       "      <td>97.41</td>\n",
       "      <td>644.8</td>\n",
       "      <td>0.10540</td>\n",
       "      <td>0.18680</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.087830</td>\n",
       "      <td>0.2252</td>\n",
       "      <td>0.06924</td>\n",
       "      <td>...</td>\n",
       "      <td>17.620</td>\n",
       "      <td>33.21</td>\n",
       "      <td>122.40</td>\n",
       "      <td>896.9</td>\n",
       "      <td>0.15250</td>\n",
       "      <td>0.66430</td>\n",
       "      <td>0.55390</td>\n",
       "      <td>0.27010</td>\n",
       "      <td>0.4264</td>\n",
       "      <td>0.12750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>18.610</td>\n",
       "      <td>20.25</td>\n",
       "      <td>122.10</td>\n",
       "      <td>1094.0</td>\n",
       "      <td>0.09440</td>\n",
       "      <td>0.10660</td>\n",
       "      <td>0.149000</td>\n",
       "      <td>0.077310</td>\n",
       "      <td>0.1697</td>\n",
       "      <td>0.05699</td>\n",
       "      <td>...</td>\n",
       "      <td>21.310</td>\n",
       "      <td>27.26</td>\n",
       "      <td>139.90</td>\n",
       "      <td>1403.0</td>\n",
       "      <td>0.13380</td>\n",
       "      <td>0.21170</td>\n",
       "      <td>0.34460</td>\n",
       "      <td>0.14900</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>0.07421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>15.300</td>\n",
       "      <td>25.27</td>\n",
       "      <td>102.40</td>\n",
       "      <td>732.4</td>\n",
       "      <td>0.10820</td>\n",
       "      <td>0.16970</td>\n",
       "      <td>0.168300</td>\n",
       "      <td>0.087510</td>\n",
       "      <td>0.1926</td>\n",
       "      <td>0.06540</td>\n",
       "      <td>...</td>\n",
       "      <td>20.270</td>\n",
       "      <td>36.71</td>\n",
       "      <td>149.30</td>\n",
       "      <td>1269.0</td>\n",
       "      <td>0.16410</td>\n",
       "      <td>0.61100</td>\n",
       "      <td>0.63350</td>\n",
       "      <td>0.20240</td>\n",
       "      <td>0.4027</td>\n",
       "      <td>0.09876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>17.570</td>\n",
       "      <td>15.05</td>\n",
       "      <td>115.00</td>\n",
       "      <td>955.1</td>\n",
       "      <td>0.09847</td>\n",
       "      <td>0.11570</td>\n",
       "      <td>0.098750</td>\n",
       "      <td>0.079530</td>\n",
       "      <td>0.1739</td>\n",
       "      <td>0.06149</td>\n",
       "      <td>...</td>\n",
       "      <td>20.010</td>\n",
       "      <td>19.52</td>\n",
       "      <td>134.90</td>\n",
       "      <td>1227.0</td>\n",
       "      <td>0.12550</td>\n",
       "      <td>0.28120</td>\n",
       "      <td>0.24890</td>\n",
       "      <td>0.14560</td>\n",
       "      <td>0.2756</td>\n",
       "      <td>0.07919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>7.691</td>\n",
       "      <td>25.44</td>\n",
       "      <td>48.34</td>\n",
       "      <td>170.4</td>\n",
       "      <td>0.08668</td>\n",
       "      <td>0.11990</td>\n",
       "      <td>0.092520</td>\n",
       "      <td>0.013640</td>\n",
       "      <td>0.2037</td>\n",
       "      <td>0.07751</td>\n",
       "      <td>...</td>\n",
       "      <td>8.678</td>\n",
       "      <td>31.89</td>\n",
       "      <td>54.49</td>\n",
       "      <td>223.6</td>\n",
       "      <td>0.15960</td>\n",
       "      <td>0.30640</td>\n",
       "      <td>0.33930</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.2790</td>\n",
       "      <td>0.10660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>11.540</td>\n",
       "      <td>14.44</td>\n",
       "      <td>74.65</td>\n",
       "      <td>402.9</td>\n",
       "      <td>0.09984</td>\n",
       "      <td>0.11200</td>\n",
       "      <td>0.067370</td>\n",
       "      <td>0.025940</td>\n",
       "      <td>0.1818</td>\n",
       "      <td>0.06782</td>\n",
       "      <td>...</td>\n",
       "      <td>12.260</td>\n",
       "      <td>19.68</td>\n",
       "      <td>78.78</td>\n",
       "      <td>457.8</td>\n",
       "      <td>0.13450</td>\n",
       "      <td>0.21180</td>\n",
       "      <td>0.17970</td>\n",
       "      <td>0.06918</td>\n",
       "      <td>0.2329</td>\n",
       "      <td>0.08134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>14.470</td>\n",
       "      <td>24.99</td>\n",
       "      <td>95.81</td>\n",
       "      <td>656.4</td>\n",
       "      <td>0.08837</td>\n",
       "      <td>0.12300</td>\n",
       "      <td>0.100900</td>\n",
       "      <td>0.038900</td>\n",
       "      <td>0.1872</td>\n",
       "      <td>0.06341</td>\n",
       "      <td>...</td>\n",
       "      <td>16.220</td>\n",
       "      <td>31.73</td>\n",
       "      <td>113.50</td>\n",
       "      <td>808.9</td>\n",
       "      <td>0.13400</td>\n",
       "      <td>0.42020</td>\n",
       "      <td>0.40400</td>\n",
       "      <td>0.12050</td>\n",
       "      <td>0.3187</td>\n",
       "      <td>0.10230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>14.740</td>\n",
       "      <td>25.42</td>\n",
       "      <td>94.70</td>\n",
       "      <td>668.6</td>\n",
       "      <td>0.08275</td>\n",
       "      <td>0.07214</td>\n",
       "      <td>0.041050</td>\n",
       "      <td>0.030270</td>\n",
       "      <td>0.1840</td>\n",
       "      <td>0.05680</td>\n",
       "      <td>...</td>\n",
       "      <td>16.510</td>\n",
       "      <td>32.29</td>\n",
       "      <td>107.40</td>\n",
       "      <td>826.4</td>\n",
       "      <td>0.10600</td>\n",
       "      <td>0.13760</td>\n",
       "      <td>0.16110</td>\n",
       "      <td>0.10950</td>\n",
       "      <td>0.2722</td>\n",
       "      <td>0.06956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>13.210</td>\n",
       "      <td>28.06</td>\n",
       "      <td>84.88</td>\n",
       "      <td>538.4</td>\n",
       "      <td>0.08671</td>\n",
       "      <td>0.06877</td>\n",
       "      <td>0.029870</td>\n",
       "      <td>0.032750</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.05781</td>\n",
       "      <td>...</td>\n",
       "      <td>14.370</td>\n",
       "      <td>37.17</td>\n",
       "      <td>92.48</td>\n",
       "      <td>629.6</td>\n",
       "      <td>0.10720</td>\n",
       "      <td>0.13810</td>\n",
       "      <td>0.10620</td>\n",
       "      <td>0.07958</td>\n",
       "      <td>0.2473</td>\n",
       "      <td>0.06443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>13.870</td>\n",
       "      <td>20.70</td>\n",
       "      <td>89.77</td>\n",
       "      <td>584.8</td>\n",
       "      <td>0.09578</td>\n",
       "      <td>0.10180</td>\n",
       "      <td>0.036880</td>\n",
       "      <td>0.023690</td>\n",
       "      <td>0.1620</td>\n",
       "      <td>0.06688</td>\n",
       "      <td>...</td>\n",
       "      <td>15.050</td>\n",
       "      <td>24.75</td>\n",
       "      <td>99.17</td>\n",
       "      <td>688.6</td>\n",
       "      <td>0.12640</td>\n",
       "      <td>0.20370</td>\n",
       "      <td>0.13770</td>\n",
       "      <td>0.06845</td>\n",
       "      <td>0.2249</td>\n",
       "      <td>0.08492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>13.620</td>\n",
       "      <td>23.23</td>\n",
       "      <td>87.19</td>\n",
       "      <td>573.2</td>\n",
       "      <td>0.09246</td>\n",
       "      <td>0.06747</td>\n",
       "      <td>0.029740</td>\n",
       "      <td>0.024430</td>\n",
       "      <td>0.1664</td>\n",
       "      <td>0.05801</td>\n",
       "      <td>...</td>\n",
       "      <td>15.350</td>\n",
       "      <td>29.09</td>\n",
       "      <td>97.58</td>\n",
       "      <td>729.8</td>\n",
       "      <td>0.12160</td>\n",
       "      <td>0.15170</td>\n",
       "      <td>0.10490</td>\n",
       "      <td>0.07174</td>\n",
       "      <td>0.2642</td>\n",
       "      <td>0.06953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>10.320</td>\n",
       "      <td>16.35</td>\n",
       "      <td>65.31</td>\n",
       "      <td>324.9</td>\n",
       "      <td>0.09434</td>\n",
       "      <td>0.04994</td>\n",
       "      <td>0.010120</td>\n",
       "      <td>0.005495</td>\n",
       "      <td>0.1885</td>\n",
       "      <td>0.06201</td>\n",
       "      <td>...</td>\n",
       "      <td>11.250</td>\n",
       "      <td>21.77</td>\n",
       "      <td>71.12</td>\n",
       "      <td>384.9</td>\n",
       "      <td>0.12850</td>\n",
       "      <td>0.08842</td>\n",
       "      <td>0.04384</td>\n",
       "      <td>0.02381</td>\n",
       "      <td>0.2681</td>\n",
       "      <td>0.07399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>10.260</td>\n",
       "      <td>16.58</td>\n",
       "      <td>65.85</td>\n",
       "      <td>320.8</td>\n",
       "      <td>0.08877</td>\n",
       "      <td>0.08066</td>\n",
       "      <td>0.043580</td>\n",
       "      <td>0.024380</td>\n",
       "      <td>0.1669</td>\n",
       "      <td>0.06714</td>\n",
       "      <td>...</td>\n",
       "      <td>10.830</td>\n",
       "      <td>22.04</td>\n",
       "      <td>71.08</td>\n",
       "      <td>357.4</td>\n",
       "      <td>0.14610</td>\n",
       "      <td>0.22460</td>\n",
       "      <td>0.17830</td>\n",
       "      <td>0.08333</td>\n",
       "      <td>0.2691</td>\n",
       "      <td>0.09479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>9.683</td>\n",
       "      <td>19.34</td>\n",
       "      <td>61.05</td>\n",
       "      <td>285.7</td>\n",
       "      <td>0.08491</td>\n",
       "      <td>0.05030</td>\n",
       "      <td>0.023370</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.1580</td>\n",
       "      <td>0.06235</td>\n",
       "      <td>...</td>\n",
       "      <td>10.930</td>\n",
       "      <td>25.59</td>\n",
       "      <td>69.10</td>\n",
       "      <td>364.2</td>\n",
       "      <td>0.11990</td>\n",
       "      <td>0.09546</td>\n",
       "      <td>0.09350</td>\n",
       "      <td>0.03846</td>\n",
       "      <td>0.2552</td>\n",
       "      <td>0.07920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>10.820</td>\n",
       "      <td>24.21</td>\n",
       "      <td>68.89</td>\n",
       "      <td>361.6</td>\n",
       "      <td>0.08192</td>\n",
       "      <td>0.06602</td>\n",
       "      <td>0.015480</td>\n",
       "      <td>0.008160</td>\n",
       "      <td>0.1976</td>\n",
       "      <td>0.06328</td>\n",
       "      <td>...</td>\n",
       "      <td>13.030</td>\n",
       "      <td>31.45</td>\n",
       "      <td>83.90</td>\n",
       "      <td>505.6</td>\n",
       "      <td>0.12040</td>\n",
       "      <td>0.16330</td>\n",
       "      <td>0.06194</td>\n",
       "      <td>0.03264</td>\n",
       "      <td>0.3059</td>\n",
       "      <td>0.07626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>10.860</td>\n",
       "      <td>21.48</td>\n",
       "      <td>68.51</td>\n",
       "      <td>360.5</td>\n",
       "      <td>0.07431</td>\n",
       "      <td>0.04227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1661</td>\n",
       "      <td>0.05948</td>\n",
       "      <td>...</td>\n",
       "      <td>11.660</td>\n",
       "      <td>24.77</td>\n",
       "      <td>74.08</td>\n",
       "      <td>412.3</td>\n",
       "      <td>0.10010</td>\n",
       "      <td>0.07348</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.2458</td>\n",
       "      <td>0.06592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>11.130</td>\n",
       "      <td>22.44</td>\n",
       "      <td>71.49</td>\n",
       "      <td>378.4</td>\n",
       "      <td>0.09566</td>\n",
       "      <td>0.08194</td>\n",
       "      <td>0.048240</td>\n",
       "      <td>0.022570</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.06552</td>\n",
       "      <td>...</td>\n",
       "      <td>12.020</td>\n",
       "      <td>28.26</td>\n",
       "      <td>77.80</td>\n",
       "      <td>436.6</td>\n",
       "      <td>0.10870</td>\n",
       "      <td>0.17820</td>\n",
       "      <td>0.15640</td>\n",
       "      <td>0.06413</td>\n",
       "      <td>0.3169</td>\n",
       "      <td>0.08032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>12.770</td>\n",
       "      <td>29.43</td>\n",
       "      <td>81.35</td>\n",
       "      <td>507.9</td>\n",
       "      <td>0.08276</td>\n",
       "      <td>0.04234</td>\n",
       "      <td>0.019970</td>\n",
       "      <td>0.014990</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.05637</td>\n",
       "      <td>...</td>\n",
       "      <td>13.870</td>\n",
       "      <td>36.00</td>\n",
       "      <td>88.10</td>\n",
       "      <td>594.7</td>\n",
       "      <td>0.12340</td>\n",
       "      <td>0.10640</td>\n",
       "      <td>0.08653</td>\n",
       "      <td>0.06498</td>\n",
       "      <td>0.2407</td>\n",
       "      <td>0.06484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>9.333</td>\n",
       "      <td>21.94</td>\n",
       "      <td>59.01</td>\n",
       "      <td>264.0</td>\n",
       "      <td>0.09240</td>\n",
       "      <td>0.05605</td>\n",
       "      <td>0.039960</td>\n",
       "      <td>0.012820</td>\n",
       "      <td>0.1692</td>\n",
       "      <td>0.06576</td>\n",
       "      <td>...</td>\n",
       "      <td>9.845</td>\n",
       "      <td>25.05</td>\n",
       "      <td>62.86</td>\n",
       "      <td>295.8</td>\n",
       "      <td>0.11030</td>\n",
       "      <td>0.08298</td>\n",
       "      <td>0.07993</td>\n",
       "      <td>0.02564</td>\n",
       "      <td>0.2435</td>\n",
       "      <td>0.07393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>12.880</td>\n",
       "      <td>28.92</td>\n",
       "      <td>82.50</td>\n",
       "      <td>514.3</td>\n",
       "      <td>0.08123</td>\n",
       "      <td>0.05824</td>\n",
       "      <td>0.061950</td>\n",
       "      <td>0.023430</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.05708</td>\n",
       "      <td>...</td>\n",
       "      <td>13.890</td>\n",
       "      <td>35.74</td>\n",
       "      <td>88.84</td>\n",
       "      <td>595.7</td>\n",
       "      <td>0.12270</td>\n",
       "      <td>0.16200</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.06493</td>\n",
       "      <td>0.2372</td>\n",
       "      <td>0.07242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>10.290</td>\n",
       "      <td>27.61</td>\n",
       "      <td>65.67</td>\n",
       "      <td>321.4</td>\n",
       "      <td>0.09030</td>\n",
       "      <td>0.07658</td>\n",
       "      <td>0.059990</td>\n",
       "      <td>0.027380</td>\n",
       "      <td>0.1593</td>\n",
       "      <td>0.06127</td>\n",
       "      <td>...</td>\n",
       "      <td>10.840</td>\n",
       "      <td>34.91</td>\n",
       "      <td>69.57</td>\n",
       "      <td>357.6</td>\n",
       "      <td>0.13840</td>\n",
       "      <td>0.17100</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0.09127</td>\n",
       "      <td>0.2226</td>\n",
       "      <td>0.08283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>10.160</td>\n",
       "      <td>19.59</td>\n",
       "      <td>64.73</td>\n",
       "      <td>311.7</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.07504</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>0.011160</td>\n",
       "      <td>0.1791</td>\n",
       "      <td>0.06331</td>\n",
       "      <td>...</td>\n",
       "      <td>10.650</td>\n",
       "      <td>22.88</td>\n",
       "      <td>67.88</td>\n",
       "      <td>347.3</td>\n",
       "      <td>0.12650</td>\n",
       "      <td>0.12000</td>\n",
       "      <td>0.01005</td>\n",
       "      <td>0.02232</td>\n",
       "      <td>0.2262</td>\n",
       "      <td>0.06742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>9.423</td>\n",
       "      <td>27.88</td>\n",
       "      <td>59.26</td>\n",
       "      <td>271.3</td>\n",
       "      <td>0.08123</td>\n",
       "      <td>0.04971</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1742</td>\n",
       "      <td>0.06059</td>\n",
       "      <td>...</td>\n",
       "      <td>10.490</td>\n",
       "      <td>34.24</td>\n",
       "      <td>66.50</td>\n",
       "      <td>330.6</td>\n",
       "      <td>0.10730</td>\n",
       "      <td>0.07158</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.2475</td>\n",
       "      <td>0.06969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>14.590</td>\n",
       "      <td>22.68</td>\n",
       "      <td>96.39</td>\n",
       "      <td>657.1</td>\n",
       "      <td>0.08473</td>\n",
       "      <td>0.13300</td>\n",
       "      <td>0.102900</td>\n",
       "      <td>0.037360</td>\n",
       "      <td>0.1454</td>\n",
       "      <td>0.06147</td>\n",
       "      <td>...</td>\n",
       "      <td>15.480</td>\n",
       "      <td>27.27</td>\n",
       "      <td>105.90</td>\n",
       "      <td>733.5</td>\n",
       "      <td>0.10260</td>\n",
       "      <td>0.31710</td>\n",
       "      <td>0.36620</td>\n",
       "      <td>0.11050</td>\n",
       "      <td>0.2258</td>\n",
       "      <td>0.08004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>11.510</td>\n",
       "      <td>23.93</td>\n",
       "      <td>74.52</td>\n",
       "      <td>403.5</td>\n",
       "      <td>0.09261</td>\n",
       "      <td>0.10210</td>\n",
       "      <td>0.111200</td>\n",
       "      <td>0.041050</td>\n",
       "      <td>0.1388</td>\n",
       "      <td>0.06570</td>\n",
       "      <td>...</td>\n",
       "      <td>12.480</td>\n",
       "      <td>37.16</td>\n",
       "      <td>82.28</td>\n",
       "      <td>474.2</td>\n",
       "      <td>0.12980</td>\n",
       "      <td>0.25170</td>\n",
       "      <td>0.36300</td>\n",
       "      <td>0.09653</td>\n",
       "      <td>0.2112</td>\n",
       "      <td>0.08732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>14.050</td>\n",
       "      <td>27.15</td>\n",
       "      <td>91.38</td>\n",
       "      <td>600.4</td>\n",
       "      <td>0.09929</td>\n",
       "      <td>0.11260</td>\n",
       "      <td>0.044620</td>\n",
       "      <td>0.043040</td>\n",
       "      <td>0.1537</td>\n",
       "      <td>0.06171</td>\n",
       "      <td>...</td>\n",
       "      <td>15.300</td>\n",
       "      <td>33.17</td>\n",
       "      <td>100.20</td>\n",
       "      <td>706.7</td>\n",
       "      <td>0.12410</td>\n",
       "      <td>0.22640</td>\n",
       "      <td>0.13260</td>\n",
       "      <td>0.10480</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>0.08321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>11.200</td>\n",
       "      <td>29.37</td>\n",
       "      <td>70.67</td>\n",
       "      <td>386.0</td>\n",
       "      <td>0.07449</td>\n",
       "      <td>0.03558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>0.05502</td>\n",
       "      <td>...</td>\n",
       "      <td>11.920</td>\n",
       "      <td>38.30</td>\n",
       "      <td>75.19</td>\n",
       "      <td>439.6</td>\n",
       "      <td>0.09267</td>\n",
       "      <td>0.05494</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.05905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>15.220</td>\n",
       "      <td>30.62</td>\n",
       "      <td>103.40</td>\n",
       "      <td>716.9</td>\n",
       "      <td>0.10480</td>\n",
       "      <td>0.20870</td>\n",
       "      <td>0.255000</td>\n",
       "      <td>0.094290</td>\n",
       "      <td>0.2128</td>\n",
       "      <td>0.07152</td>\n",
       "      <td>...</td>\n",
       "      <td>17.520</td>\n",
       "      <td>42.79</td>\n",
       "      <td>128.70</td>\n",
       "      <td>915.0</td>\n",
       "      <td>0.14170</td>\n",
       "      <td>0.79170</td>\n",
       "      <td>1.17000</td>\n",
       "      <td>0.23560</td>\n",
       "      <td>0.4089</td>\n",
       "      <td>0.14090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>20.920</td>\n",
       "      <td>25.09</td>\n",
       "      <td>143.00</td>\n",
       "      <td>1347.0</td>\n",
       "      <td>0.10990</td>\n",
       "      <td>0.22360</td>\n",
       "      <td>0.317400</td>\n",
       "      <td>0.147400</td>\n",
       "      <td>0.2149</td>\n",
       "      <td>0.06879</td>\n",
       "      <td>...</td>\n",
       "      <td>24.290</td>\n",
       "      <td>29.41</td>\n",
       "      <td>179.10</td>\n",
       "      <td>1819.0</td>\n",
       "      <td>0.14070</td>\n",
       "      <td>0.41860</td>\n",
       "      <td>0.65990</td>\n",
       "      <td>0.25420</td>\n",
       "      <td>0.2929</td>\n",
       "      <td>0.09873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.560</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.243900</td>\n",
       "      <td>0.138900</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.41070</td>\n",
       "      <td>0.22160</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.130</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.144000</td>\n",
       "      <td>0.097910</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.32150</td>\n",
       "      <td>0.16280</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.600</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.092510</td>\n",
       "      <td>0.053020</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.34030</td>\n",
       "      <td>0.14180</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.600</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.351400</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.93870</td>\n",
       "      <td>0.26500</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.760</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
       "0                   0.07871  ...        25.380          17.33   \n",
       "1                   0.05667  ...        24.990          23.41   \n",
       "2                   0.05999  ...        23.570          25.53   \n",
       "3                   0.09744  ...        14.910          26.50   \n",
       "4                   0.05883  ...        22.540          16.67   \n",
       "..                      ...  ...           ...            ...   \n",
       "564                 0.05623  ...        25.450          26.40   \n",
       "565                 0.05533  ...        23.690          38.25   \n",
       "566                 0.05648  ...        18.980          34.12   \n",
       "567                 0.07016  ...        25.740          39.42   \n",
       "568                 0.05884  ...         9.456          30.37   \n",
       "\n",
       "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
       "0             184.60      2019.0           0.16220            0.66560   \n",
       "1             158.80      1956.0           0.12380            0.18660   \n",
       "2             152.50      1709.0           0.14440            0.42450   \n",
       "3              98.87       567.7           0.20980            0.86630   \n",
       "4             152.20      1575.0           0.13740            0.20500   \n",
       "..               ...         ...               ...                ...   \n",
       "564           166.10      2027.0           0.14100            0.21130   \n",
       "565           155.00      1731.0           0.11660            0.19220   \n",
       "566           126.70      1124.0           0.11390            0.30940   \n",
       "567           184.60      1821.0           0.16500            0.86810   \n",
       "568            59.16       268.6           0.08996            0.06444   \n",
       "\n",
       "     worst concavity  worst concave points  worst symmetry  \\\n",
       "0             0.7119                0.2654          0.4601   \n",
       "1             0.2416                0.1860          0.2750   \n",
       "2             0.4504                0.2430          0.3613   \n",
       "3             0.6869                0.2575          0.6638   \n",
       "4             0.4000                0.1625          0.2364   \n",
       "..               ...                   ...             ...   \n",
       "564           0.4107                0.2216          0.2060   \n",
       "565           0.3215                0.1628          0.2572   \n",
       "566           0.3403                0.1418          0.2218   \n",
       "567           0.9387                0.2650          0.4087   \n",
       "568           0.0000                0.0000          0.2871   \n",
       "\n",
       "     worst fractal dimension  \n",
       "0                    0.11890  \n",
       "1                    0.08902  \n",
       "2                    0.08758  \n",
       "3                    0.17300  \n",
       "4                    0.07678  \n",
       "..                       ...  \n",
       "564                  0.07115  \n",
       "565                  0.06637  \n",
       "566                  0.07820  \n",
       "567                  0.12400  \n",
       "568                  0.07039  \n",
       "\n",
       "[569 rows x 30 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The following cell would plot a histogram of `mean radius` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd7a2f172e8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cancer_df['mean radius'].hist(bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The following cell would plot a histogram of `mean radius` values across the two different classes with two different colors. Notice that it is a similar histogram, but can tell us the frequency of `mean radius` values across the binary classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for class_number in np.unique(cancer_data.target):\n",
    "    plt.figure(1)\n",
    "    cancer_df['mean radius'].iloc[np.where(cancer_data.target == class_number)[0]].hist(bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, it looks like that `mean radius` is highly correlated with the class labels, there is a clear distinction in two classes in terms of the mean radius. Data visualization often helps us get better insights about the data before we tackle to solve the ML problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we can split the data. The split ratio we are going to choose is 0.7 for training and 0.3 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(398, 30)\n",
      "(171, 30)\n",
      "(398,)\n",
      "(171,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.30, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, we can now choose which classifier from the built-in classifiers in sklearn we want to choose. We are going to use three classifiers: Stochastic Gradient Descent Classifier (SGD), Logist Regression and Support Vector Machines (SVM). We start with SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Required Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "### START CODING HERE ###\n",
    "# Import necessary module from sklearn.linear_model\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Call SGDClassifier with a random_state of 42\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "\n",
    "# Fit the model to X_train and y_train\n",
    "sgd_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on X_test and store the results in y_pred\n",
    "y_pred = sgd_clf.predict(X_test)\n",
    "\n",
    "### END CODING HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Now, evaluate your model using the metrics you've learnt in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Required Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score before:  0.9600000000000001\n",
      "auc before:  0.9285714285714286\n",
      "cross validation before:  0.8574066948826472\n",
      "f1_score after:  0.9727272727272727\n",
      "auc after:  0.9556878306878308\n",
      "cross validation after:  0.8119738360908041\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "### START CODING HERE ###\n",
    "# Import the necessary modules from sklearn for f1_score, roc_auc_score and cross_val_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Compute the f1_score, roc_auc_score and cross_val_score and store them in properly named variables: \n",
    "# f1_score_sgd, auc_sgd, cv_sgd\n",
    "# Hint: for cross validation with 5 folds, you should call the method on\n",
    "# (sgd_clf, cancer_data.data, cancer_data.target, cv=5)\n",
    "f1_score_sgd = f1_score(y_test, y_pred)\n",
    "auc_sgd = roc_auc_score(y_test, y_pred)\n",
    "cv_sgd = cross_val_score(sgd_clf, cancer_data.data, cancer_data.target, cv=5)\n",
    "None\n",
    "None\n",
    "### END CODING HERE ###\n",
    "\n",
    "# Print the performance measures\n",
    "print(\"f1_score before: \", f1_score_sgd)\n",
    "print(\"auc before: \", auc_sgd)\n",
    "print(\"cross validation before: \", cv_sgd.mean())\n",
    "\n",
    "# Import necessary module from sklearn.linear_model\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Call SGDClassifier with a random_state of 42\n",
    "sgd_clf = SGDClassifier(random_state=43, early_stopping=False, shuffle=False)\n",
    "\n",
    "# Fit the model to X_train and y_train\n",
    "sgd_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on X_test and store the results in y_pred\n",
    "y_pred = sgd_clf.predict(X_test)\n",
    "\n",
    "f1_score_sgd = f1_score(y_test, y_pred)\n",
    "auc_sgd = roc_auc_score(y_test, y_pred)\n",
    "cv_sgd = cross_val_score(sgd_clf, cancer_data.data, cancer_data.target, cv=5)\n",
    "None\n",
    "None\n",
    "### END CODING HERE ###\n",
    "\n",
    "# Print the performance measures\n",
    "print(\"f1_score after: \", f1_score_sgd)\n",
    "print(\"auc after: \", auc_sgd)\n",
    "print(\"cross validation after: \", cv_sgd.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Now, let's try logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Required Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:  0.9724770642201834\n",
      "auc:  0.958994708994709\n",
      "cross validation:  0.9526433243555215\n",
      "f1_score after:  0.9814814814814815\n",
      "auc after:  0.9748677248677248\n",
      "cross validation after:  0.9508426317814542\n"
     ]
    }
   ],
   "source": [
    "### START CODING HERE ###\n",
    "# Import necessary module from sklearn.linear_model for Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Call LogisticRegression with a random_state of 0, solver='liblinear'\n",
    "logreg_clf = LogisticRegression(random_state=0, solver='liblinear')\n",
    "\n",
    "# Fit the model to X_train and y_train\n",
    "logreg_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on X_test and store the results in y_pred\n",
    "y_pred = logreg_clf.predict(X_test)\n",
    "\n",
    "# Compute the f1_score, roc_auc_score and cross_val_score and store them in properly named variables: \n",
    "# f1_score_logreg, auc_logreg, cv_logreg\n",
    "# Hint: for cross validation with 5 folds, you should call the method on\n",
    "# (logreg_clf, cancer_data.data, cancer_data.target, cv=5)\n",
    "f1_score_logreg = f1_score(y_test, y_pred)\n",
    "auc_logreg = roc_auc_score(y_test, y_pred)\n",
    "cv_logreg = cross_val_score(logreg_clf, cancer_data.data, cancer_data.target, cv=5)\n",
    "### END CODING HERE ###\n",
    "\n",
    "# Print the performance measures\n",
    "print(\"f1_score: \", f1_score_logreg)\n",
    "print(\"auc: \", auc_logreg)\n",
    "print(\"cross validation: \", cv_logreg.mean())\n",
    "\n",
    "logreg_clf = LogisticRegression(random_state=1, solver='newton-cg', max_iter=10000)\n",
    "\n",
    "# Fit the model to X_train and y_train\n",
    "logreg_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on X_test and store the results in y_pred\n",
    "y_pred = logreg_clf.predict(X_test)\n",
    "\n",
    "# Compute the f1_score, roc_auc_score and cross_val_score and store them in properly named variables: \n",
    "# f1_score_logreg, auc_logreg, cv_logreg\n",
    "# Hint: for cross validation with 5 folds, you should call the method on\n",
    "# (logreg_clf, cancer_data.data, cancer_data.target, cv=5)\n",
    "f1_score_logreg = f1_score(y_test, y_pred)\n",
    "auc_logreg = roc_auc_score(y_test, y_pred)\n",
    "cv_logreg = cross_val_score(logreg_clf, cancer_data.data, cancer_data.target, cv=5)\n",
    "### END CODING HERE ###\n",
    "\n",
    "# Print the performance measures\n",
    "print(\"f1_score after: \", f1_score_logreg)\n",
    "print(\"auc after: \", auc_logreg)\n",
    "print(\"cross validation after: \", cv_logreg.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like Logistic Regression performed better on this dataset so far, although without parameter tuning. Can you improve your reults by tuning the parameters of SGD (any parameter) or parameters of Logistic Regression? You can check the full list of parameters for each classifier in its sklearn documentation page, [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) and [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). This kind of parameter settings is a brute-force or grid-search strategy. You need to try changing at least three parameters, but only report the results below if you noticed any improvements higher than 1% in any of the scores.\n",
    "\n",
    "Report your results HERE:\n",
    "For SGDClassifier, I changed the random_state and I changed the shuffle parameter. Shuffle is the parameter that says whether or not the training data should be shuffled after each epoch. It is set to true by default so I set it to false. I also set the random_state to 43 instead of 42. This increased the f1_score and auc score by about 2% and 3% respectively. For Logistic Regression, I changed the random_state, solver, and max_iterations. I changed the solver to newton-cg and it added about 2% to auc and 1% to f1. The other parameters affected the scores very minimally but I changed them nonetheless. I just duplicated the code and pasted it below to visualize the different parameters that I changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's try SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Required Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:  0.9767441860465117\n",
      "auc:  0.9702380952380952\n",
      "cross validation:  0.9508426317814542\n"
     ]
    }
   ],
   "source": [
    "# Import necessary module from sklearn for svm\n",
    "from sklearn import svm\n",
    "\n",
    "### START CODING HERE ###\n",
    "# Call svm.SVC with kernel='linear', gamma=10, C=10\n",
    "svm_clf = svm.SVC(kernel = 'linear', gamma = 10, C=10)\n",
    "\n",
    "# Fit the model to X_train and y_train\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on X_test and store the results in y_pred\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "\n",
    "# Compute the f1_score, roc_auc_score and cross_val_score and store them in properly named variables: \n",
    "# f1_score_svm, auc_svm, cv_svm\n",
    "# Hint: for cross validation with 5 folds, you should call the method on\n",
    "# (svm_clf, cancer_data.data, cancer_data.target, cv=5)\n",
    "f1_score_svm = f1_score(y_test, y_pred)\n",
    "auc_svm = roc_auc_score(y_test, y_pred)\n",
    "cv_svm = cross_val_score(logreg_clf, cancer_data.data, cancer_data.target, cv=5)\n",
    "### END CODING HERE ###\n",
    "\n",
    "# Print the performance measures\n",
    "print(\"f1_score: \", f1_score_svm)\n",
    "print(\"auc: \", auc_svm)\n",
    "print(\"cross validation: \", cv_svm.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Now, change the svm kernel to `'poly'` and `'rbf'` which are non-linear kernels, and see if they improve or hurt the results or have no significant impact. Report your results HERE:\n",
    "\n",
    "Both poly and rbf did not import the results. Linear endedup being the best in terms of the scores that I calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How about parameter C? Does it have any impact on the performance? Complete the following cell and plot the impact of varying C in range `[1..100]` on f1_score. Once implemented, running this cell may take a few minutes to generate the correct plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Required Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd7a0863be0>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create two lists for C_values and f_scores\n",
    "C_values = range(1, 100)\n",
    "f_scores = []\n",
    "\n",
    "### START CODING HERE ###\n",
    "# Write a for loop that does the following steps:\n",
    "# iterate over c in C_values\n",
    "for c in C_values:\n",
    "    # call svm model in each iteration passing a linear kernel, gamma=10 and the current c\n",
    "    svm_clf = svm.SVC(kernel = 'linear', gamma = 10, C=c)\n",
    "    # fit the model on X_train and y-train\n",
    "    svm_clf.fit(X_train, y_train)\n",
    "    # predict X_test and store them in y_pred\n",
    "    y_pred = svm_clf.predict(X_test)\n",
    "    # compute f1_score and append it to f_scores\n",
    "    f_scores.append(f1_score(y_test, y_pred))\n",
    "### END CODING HERE ###\n",
    "\n",
    "plt.title('Impact of SVM C Parameter on f1_score')\n",
    "plt.xlabel('C', fontsize=14)\n",
    "plt.ylabel('f1_score', fontsize=14)\n",
    "plt.plot(C_values, f_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following question HERE:\n",
    "\n",
    "Q1- Based on your plot, what ranges/values for C are optimal? What ranges/values may not be so good?\n",
    "    Based on the plot, values of C that are optimal are between 0 - 10. Anything less than that is less optimal but I would definetly not use values of C that approaching 100.\n",
    "\n",
    "Q2- What happens to f1_score for C values closer to 100? Compared to C=1, would the SVM street be wider or narrower?\n",
    "    The f1_score decreases for C values closer to 100. The SVM street would be narrower because the c value denotes how it wants to correctly seperate values. A high value of c in turn creates a narrow margin.\n",
    "\n",
    "Q3- Why do you fit the classifier for cross_val_score on the whole dataset instead of `(X_train, y_train)` only?\n",
    "    We do this because it is part of the k-folds cross valuation test. We use k-folds to decrease bias and ensure that all data points have a chance to appear in the test and training data. This ensures we have the best training and testing data to use the model on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II - Image Classification with SVM -  <font color=\"green\"> Optional - Extra Credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and extract [CIFAR-10 dataset](https://www.cs.utoronto.ca/~kriz/cifar.html). It consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. \n",
    "\n",
    "The required steps in the pipeline of Part II:\n",
    "\n",
    "- 1- Load the image dataset using the provided helper functions in CIFAR website or the functions you write.\n",
    "\n",
    "- 2- Train a linear or non-linear SVM classifier that can classify the images.\n",
    "\n",
    "- 3- Fine-tune your model to improve your results.\n",
    "\n",
    "You may also search for open-source solutions and reproduce their results provided that you do **ALL** of the followings:\n",
    "\n",
    "- Properly cite and reference any source or website you've consulted with.\n",
    "\n",
    "- Only use SVMs for this part of the assignment.\n",
    "\n",
    "- Include a section in your notebook written by yourself for fine-tuning the SVM parameters to improve the available benchmark results of CIFAR-10.\n",
    "\n",
    "You should include all of the PART II code in THIS notebook.\n",
    "\n",
    "If your model meets **ALL** of the requirements above, at my discretion and judgement of your work, you may be rewarded with extra credit equivalent to up to 50% of what Assignment-2 is worth in the final weighting/grading of the Assignment part of your final grade. This could be an opportunity to reserve some points for the points you may lose in your future assignments. Assignment-2 extra credit **can't** be used to compensate for other parts of your grade such as quiz or exams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading\n",
    "\n",
    "For assignment 2, your notebook will be run and graded manually with a maximum of 100 points. Make sure that you get the correct outputs for all cells that you implement. Also, your notebook should be written with no grammatical and spelling errors and should be nicely-formatted and easy-to-read.\n",
    "\n",
    "The breakdown of the 100 points is as follows:\n",
    "\n",
    "Part I implementaion has 75 points:\n",
    "- 15 points per SGD and Logistic Regression (total 30 points): correct implementation for each classifier and getting correct score results\n",
    "- 20 points: correct SVM implementation and results\n",
    "- 25 points: correct plot of C vs f1_score\n",
    "\n",
    "Part I questions have 15 points (5 points each).\n",
    "\n",
    "The remaining 10 points will be based on your writing and formatting as instructed in the notebook.  Follow the instructions of each section carefully. Points will be deducted if your submitted notebook is not easy to read and follow or if it has grammatical and spelling errors.\n",
    "\n",
    "Part II is optional and is an extra credit opportunity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Submit and Due Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name your notebook ```Lastname-A2.ipynb```.  So, for me it would be ```Vafaei-A2.ipynb```.  Submit the file using the ```Assignment-2``` link on Blackboard.\n",
    "\n",
    "Grading will be based on \n",
    "\n",
    "  * correct behavior of the required functions, correct answer to the questions, and\n",
    "  * readability of the notebook.\n",
    "  \n",
    "<font color=red><b>Due Date: Monday Oct 14th 11:59PM.</b></font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
